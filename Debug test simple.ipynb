{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"13eLU7yAhMHwZLm9MkbdnNwHwIzA2bEFl","authorship_tag":"ABX9TyOPMLU2kdq52IanThlwgjGY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## A image classification pipeline with PyTorch, model can be a single layer Conv2d.\n","This example uses torchvision.datasets.FakeData as a placeholder dataset (you can replace it with CIFAR-10, MNIST, or your own)."],"metadata":{"id":"Izs-nbZqnM-E"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n"],"metadata":{"id":"eLm0Nvp2nQp9","executionInfo":{"status":"ok","timestamp":1749491983395,"user_tz":420,"elapsed":10,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nAe4q8er_CI","executionInfo":{"status":"ok","timestamp":1749491984163,"user_tz":420,"elapsed":766,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}},"outputId":"92bc883f-1c01-41a1-8462-9c9e04ac2e2a"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#Define the model\n","class SingleConvNet(nn.Module):\n","  def __init__(self, in_channels=1, num_classes=10):\n","    super().__init__()\n","    self.conv = nn.Conv2d(in_channels, 16, kernel_size=3, padding=1)\n","    self.pool = nn.AdaptiveAvgPool2d((1, 1))\n","    self.classifier = nn.Linear(16, num_classes)\n","\n","  def forward(self, x):\n","    x = self.conv(x)          # (B, 16, 28, 28)\n","    x = torch.relu(x)\n","    x = self.pool(x)          # (B, 16, 1, 1)\n","    x = torch.flatten(x, 1)   # (B, 16)\n","    return self.classifier(x) # (B, 10)"],"metadata":{"id":"bMnNt3XynQ59","executionInfo":{"status":"ok","timestamp":1749491984163,"user_tz":420,"elapsed":4,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["## If we want to add a deeper layer"],"metadata":{"id":"rAM10aMw-3tE"}},{"cell_type":"code","source":[" # First convolutional block\n","    self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)  # Output: (B, 32, 28, 28)\n","    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)                 # Output: (B, 32, 14, 14)\n","\n","    # Second convolutional block\n","    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)           # Output: (B, 64, 14, 14)\n","    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)                 # Output: (B, 64, 7, 7)\n","\n","    # Fully connected layers\n","    self.fc1 = nn.Linear(64 * 7 * 7, 128)\n","    self.fc2 = nn.Linear(128, num_classes)\n","\n","def forward(self, x):\n","    # Convolutional layers with ReLU and pooling\n","    x = F.relu(self.conv1(x))\n","    x = self.pool1(x)\n","\n","    x = F.relu(self.conv2(x))\n","    x = self.pool2(x)\n","\n","    # Flatten\n","    x = x.view(x.size(0), -1)  # or torch.flatten(x, 1)\n","\n","    # Fully connected layers\n","    x = F.relu(self.fc1(x))\n","    x = self.fc2(x)\n","\n","    return x"],"metadata":{"id":"cDs-Chjb-3YM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset and Data loader"],"metadata":{"id":"3LhD7MgtnoWe"}},{"cell_type":"code","source":["#Dataset and data loader\n","\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset = datasets.FakeData(size=1000, image_size=(3, 32, 32), num_classes=10, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"CWNWwOKdnQ8l","executionInfo":{"status":"ok","timestamp":1749491984164,"user_tz":420,"elapsed":3,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["#Use MNIST for testing\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"EBURS-mkoY2c","executionInfo":{"status":"ok","timestamp":1749491984246,"user_tz":420,"elapsed":84,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["## Training loop"],"metadata":{"id":"XLsWd--MnlyW"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = SingleConvNet().to(device)\n","\n","# To use a deeper model with your training loop: run this, then continue with loss function, optimizer, and training loop as before.\n","#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#model = DeepConvNet().to(device)\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","for epoch in range(5):\n","    model.train()\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gY_QKK7nlCN","executionInfo":{"status":"ok","timestamp":1749492054234,"user_tz":420,"elapsed":69987,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}},"outputId":"0151ad2d-69aa-41ee-ba3f-7913e2e194b4"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 2.1727\n","Epoch 2, Loss: 2.0423\n","Epoch 3, Loss: 2.0267\n","Epoch 4, Loss: 2.0088\n","Epoch 5, Loss: 1.9768\n"]}]},{"cell_type":"markdown","source":["## Test evaluation\n","add a test set in a similar way with model.eval() and no gradients"],"metadata":{"id":"JtEROsvGnOT2"}},{"cell_type":"code","source":["def evaluate(model, loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = outputs.max(1)\n","            correct += (predicted == labels).sum().item()\n","            total += labels.size(0)\n","    print(f\"Accuracy: {100 * correct / total:.2f}%\")"],"metadata":{"id":"YAdbHwkNn1oU","executionInfo":{"status":"ok","timestamp":1749492054236,"user_tz":420,"elapsed":30,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["#Add this after training\n","\n","test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","evaluate(model, test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PXWAocMioo6z","executionInfo":{"status":"ok","timestamp":1749492055403,"user_tz":420,"elapsed":1192,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}},"outputId":"b46c2666-4bf9-4e72-a2f0-7513b0d29975"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 30.22%\n"]}]},{"cell_type":"code","source":["# import os\n","# notebook_path = os.getcwd()\n","# print(notebook_path)\n","\n","# path = \"/content/drive\" # Replace with the path you want to check\n","# os.path.ismount(path)"],"metadata":{"id":"dvA5KuKJouaT","executionInfo":{"status":"ok","timestamp":1749492055409,"user_tz":420,"elapsed":4,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"HfGIhkpf-h81"}},{"cell_type":"markdown","source":["## Load PNG dataset and resize\n","\n","If you have raw .png images (like MNIST digits stored as PNGs), here's a PyTorch-compatible pipeline to:\n","\n","Load the image from disk\n","\n","Transform it to tensor\n","\n","Normalize / resize if needed\n","\n","Feed it into your SingleConvNet"],"metadata":{"id":"L84dNsAlovD9"}},{"cell_type":"code","source":["from PIL import Image\n","import torch\n","from torchvision import transforms\n","\n","# 1. Define transform (for MNIST-like PNGs)\n","transform = transforms.Compose([\n","    transforms.Grayscale(),        # ensure it's single-channel\n","    transforms.Resize((28, 28)),   # resize to 28x28 if needed\n","    transforms.ToTensor(),         # convert to tensor in [0,1], shape (1, 28, 28)\n","])\n","\n","# 2. Load PNG image\n","#image_path =  '/content/drive/My Drive/Machine learning and DL/CNN_MNP_GUV/PE_no lipids_1hr/0301-8-nolipids-1hr-scale0000.png'\n","image_path =  '/content/drive/My Drive/Machine learning and DL/CNN_MNP_GUV/number2.png'\n","\n","img = Image.open(image_path).convert('RGB')  # or 'L' for grayscale\n","\n","# 3. Apply transform\n","tensor_img = transform(img)  # shape: (1, 28, 28)\n","\n","# 4. Add batch dimension and move to device\n","input_tensor = tensor_img.unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 5. Model inference\n","model.eval()\n","with torch.no_grad():\n","    output = model(input_tensor)\n","    predicted_label = output.argmax(dim=1).item()\n","\n","print(f\"Predicted Label: {predicted_label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CeRwF-20o0WN","executionInfo":{"status":"ok","timestamp":1749492055897,"user_tz":420,"elapsed":484,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}},"outputId":"c0604d14-8680-4911-ab2c-174ac3c22e72"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Label: 1\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"LDJTKPws1p8X"}},{"cell_type":"markdown","source":["## Notes\n",".unsqueeze(0) adds the batch dimension: from (1, 28, 28) to (1, 1, 28, 28)\n","\n","If your model expects RGB (unlikely for MNIST), skip transforms.Grayscale()\n","\n","If your PNGs are already 28Ã—28 grayscale, just use ToTensor() safely\n","\n","Next; a batch loader for multiple PNG files in a folder."],"metadata":{"id":"GrrvoQIAo6Z1"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0b82d869","executionInfo":{"status":"ok","timestamp":1749492056719,"user_tz":420,"elapsed":818,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}},"outputId":"244359f2-08c4-4c1e-89ec-3b7682cc77f3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"363617eb"},"source":["After running the cell above and following the instructions to mount your Google Drive, your files will be accessible under `/content/drive/My Drive/`. You'll need to update the `image_path` in the code to reflect the actual location of your image file within your Google Drive.\n","\n","For example, if you uploaded the image to a folder named `Training data` in your Google Drive, the path might look like this:"]},{"cell_type":"code","metadata":{"id":"3bcf4e53","executionInfo":{"status":"ok","timestamp":1749492056731,"user_tz":420,"elapsed":6,"user":{"displayName":"Peiyao Wu","userId":"15716366120125129522"}}},"source":["image_path = '/content/drive/My Drive/Training data/PE_no lipids_1hr/0301-8-nolipids-1hr-scale0000.png'"],"execution_count":50,"outputs":[]}]}