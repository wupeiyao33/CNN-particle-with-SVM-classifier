{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "28vWq6ZhLC17"
      },
      "outputs": [],
      "source": [
        "# necessary libraries\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from IPython.display import display\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xowvj8YvRY9",
        "outputId": "dbf6b6a1-193f-4598-d0f3-acfde08ae3ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1t3JAksTvZdF"
      },
      "outputs": [],
      "source": [
        "class VGG16Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=10, in_channels=3, num_conv_layers_to_use=None):\n",
        "        super(VGG16Classifier, self).__init__()\n",
        "        self.vgg16 = models.vgg16(weights='DEFAULT')\n",
        "\n",
        "        for param in self.vgg16.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Determine which convolutional layers to use\n",
        "        if num_conv_layers_to_use is not None:\n",
        "            self.features = nn.Sequential(*list(self.vgg16.features.children())[:num_conv_layers_to_use])\n",
        "        else:\n",
        "            self.features = self.vgg16.features\n",
        "\n",
        "        dummy_input = torch.randn(1, in_channels, 224, 224) # Assuming input image size 224x224\n",
        "        with torch.no_grad():\n",
        "            features_output_shape = self.features(dummy_input).shape\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "        with torch.no_grad():\n",
        "             flattened_size = torch.flatten(self.avgpool(self.features(dummy_input)), 1).shape[1]\n",
        "\n",
        "\n",
        "        self.vgg16.classifier = nn.Sequential(\n",
        "            nn.Linear(flattened_size, 512), # Use the dynamically calculated flattened_size\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x) # Apply the adaptive average pooling\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.vgg16.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_BiGylEvhfd",
        "outputId": "c314f83d-88ff-4a78-ace0-2d70d51e1f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_no lipids_1hr/Copy of 130514.094.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_no lipids_1hr/Copy of 0301-8-nolipids-1hr-scale0000.png preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_no lipids_1hr/Copy of 133521.785.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_no lipids_1hr/Copy of 140743.293.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_no lipids_1hr/Copy of 171352.302.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_no lipids_1hr/Copy of 164134.452.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_no lipids_1hr/Copy of 164759.745.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_no lipids_1hr/Copy of 164155.858.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_no lipids_1hr/Copy of 171420.973.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_with lipids_1hr/Copy of 172957.297.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_with lipids_1hr/Copy of 172959.108.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_with lipids_1hr/Copy of 165702.153.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_with lipids_1hr/Copy of 0301-9-withlipids-1hr-scale0002.png preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_with lipids_1hr/Copy of 153949.774.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_with lipids_1hr/Copy of 154029.824.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_with lipids_1hr/Copy of 173749.043.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PE_1um_PBSxHEX_with lipids_1hr/Copy of 125319.359.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PS_1um_PBSxHEX_no lipids/Copy of 172239.693.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PS_1um_PBSxHEX_no lipids/Copy of Image0009.png preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PS_1um_PBSxHEX_no lipids/Copy of 122551.040.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PS_1um_PBSxHEX_no lipids/Copy of 0104-buffer120001.png preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PS_1um_PBSxHEX_no lipids/Copy of 171827.481.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PS_1um_PBSxHEX_no lipids/Copy of Image0181.png preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PS_1um_PBSxHEX_no lipids/Copy of 0104-buffer120032.png preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PS_1um_PBSxHEX_no lipids/Copy of 160725.776.bmp preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PS_1um_PBSxHEX_no lipids/Copy of 0104-buffer110182.png preprocessed\n",
            "File /content/drive/My Drive/Frechette Lab/train_dataset/PS_1um_PBSxHEX_no lipids/Copy of 163302.099.bmp preprocessed\n"
          ]
        }
      ],
      "source": [
        "output_base_dir = '/content/drive/My Drive/Frechette Lab/train_processed_dataset'\n",
        "path_list = glob.glob('/content/drive/My Drive/Frechette Lab/train_dataset/*/*')\n",
        "\n",
        "def preprocess(image_path, output, crop_height=224, crop_width=224):\n",
        "    img = Image.open(image_path)\n",
        "    width, height = img.size\n",
        "    rows = height // crop_height\n",
        "    cols = width // crop_width\n",
        "\n",
        "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "    ext = os.path.splitext(os.path.basename(image_path))[1]\n",
        "\n",
        "    class_name = os.path.basename(os.path.dirname(image_path))\n",
        "\n",
        "    output_dir = os.path.join(output, class_name)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            left = j * crop_width\n",
        "            upper = i * crop_height\n",
        "            right = left + crop_width\n",
        "            lower = upper + crop_height\n",
        "            cropped_img = img.crop((left, upper, right, lower))\n",
        "            new_filename = f\"{base_name}_{i}_{j}{ext}\"\n",
        "            save_path = os.path.join(output_dir, new_filename)\n",
        "            cropped_img.save(save_path)\n",
        "\n",
        "for y in path_list:\n",
        "    preprocess(y, output = output_base_dir, crop_height=448, crop_width=448)\n",
        "    print(f\"File {y} preprocessed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7AO4gr2vkk1",
        "outputId": "c3bb0ba3-cd77-41e2-aaef-f4fd2fd4b632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 162 images belonging to 3 classes.\n",
            "DataLoader created successfully.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    train_dataset = datasets.ImageFolder(root=output_base_dir, transform = transforms.ToTensor())\n",
        "    print(f\"Found {len(train_dataset)} images belonging to {len(train_dataset.classes)} classes.\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    print(\"DataLoader created successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error creating ImageFolder dataset: {e}\")\n",
        "    print(\"Please ensure the directory structure in\")\n",
        "    print(f\"{output_base_dir} follows the ImageFolder convention (subdirectories for classes).\")\n",
        "    print(\"Also check file permissions and that the directory exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iFfUjjGAvoAI"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsLjU5Q0vq2a",
        "outputId": "5e23e3fb-ab4f-41bd-a8fc-062057dd81ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Weights file not found at /content/drive/My Drive/Frechette Lab/VGG16Classifier_weights.pt. Starting training with random initialization.\n",
            "Starting training...\n",
            "Epoch 1, Loss: 1.5146, Test Accuracy: 30.00%\n",
            "Epoch 2, Loss: 1.2303, Test Accuracy: 40.00%\n",
            "Epoch 3, Loss: 1.1817, Test Accuracy: 30.00%\n",
            "Epoch 4, Loss: 1.3190, Test Accuracy: 40.00%\n",
            "Epoch 5, Loss: 1.2902, Test Accuracy: 30.00%\n",
            "Epoch 6, Loss: 1.2055, Test Accuracy: 40.00%\n",
            "Epoch 7, Loss: 1.4051, Test Accuracy: 40.00%\n",
            "Epoch 8, Loss: 1.2835, Test Accuracy: 40.00%\n",
            "Epoch 9, Loss: 1.2345, Test Accuracy: 30.00%\n",
            "Epoch 10, Loss: 1.2543, Test Accuracy: 30.00%\n",
            "Epoch 11, Loss: 1.2826, Test Accuracy: 20.83%\n",
            "Epoch 12, Loss: 1.1907, Test Accuracy: 30.00%\n",
            "Epoch 13, Loss: 1.0855, Test Accuracy: 30.00%\n",
            "Epoch 14, Loss: 1.1432, Test Accuracy: 30.00%\n",
            "Epoch 15, Loss: 1.1344, Test Accuracy: 40.00%\n",
            "Epoch 16, Loss: 1.1626, Test Accuracy: 30.00%\n",
            "Epoch 17, Loss: 1.2681, Test Accuracy: 40.00%\n",
            "Epoch 18, Loss: 1.1917, Test Accuracy: 30.00%\n",
            "Epoch 19, Loss: 1.1392, Test Accuracy: 30.00%\n",
            "Epoch 20, Loss: 1.1183, Test Accuracy: 30.00%\n",
            "Epoch 21, Loss: 1.2121, Test Accuracy: 40.00%\n",
            "Epoch 22, Loss: 1.1487, Test Accuracy: 40.00%\n"
          ]
        }
      ],
      "source": [
        "# Track epoch loss and accuracy for plotting\n",
        "epoch_losses = []\n",
        "epoch_accuracies = []\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# which model to use\n",
        "model_dict = {\n",
        "    'VGG16Classifier': {\n",
        "        'path': \"/content/drive/My Drive/Frechette Lab/VGG16Classifier_weights.pt\",\n",
        "        'model': VGG16Classifier(num_conv_layers_to_use=2).to(device)\n",
        "    }\n",
        "}\n",
        "\n",
        "WEIGHTS_PATH = model_dict['VGG16Classifier']['path']\n",
        "model = model_dict['VGG16Classifier']['model']\n",
        "\n",
        "# Load saved weights\n",
        "try:\n",
        "    state_dict = torch.load(WEIGHTS_PATH, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(f\"Successfully loaded model weights from {WEIGHTS_PATH}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Weights file not found at {WEIGHTS_PATH}. Starting training with random initialization.\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error loading state_dict: {e}\")\n",
        "    print(\"This might happen if the model architecture does not match the saved weights.\")\n",
        "    print(\"Starting training with random initialization.\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "TEST_PATH = glob.glob('/content/drive/My Drive/Frechette Lab/test_dataset/*/*')\n",
        "test_output = '/content/drive/My Drive/Frechette Lab/test_processed_dataset'\n",
        "for image in TEST_PATH:\n",
        "    preprocess(image, output = test_output)\n",
        "test_dataset = datasets.ImageFolder(root=test_output, transform=transforms.ToTensor())\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    epoch_losses.append(avg_loss)\n",
        "\n",
        "    #evaluate\n",
        "    accuracy = evaluate(model, test_loader)\n",
        "    epoch_accuracies.append(accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Save updated weights\n",
        "try:\n",
        "    torch.save(model.state_dict(), WEIGHTS_PATH)\n",
        "    print(f\"Successfully saved updated model weights to {WEIGHTS_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model weights: {e}\")\n",
        "\n",
        "epochs = range(1, len(epoch_losses) + 1)\n",
        "\n",
        "#Plot the loss over epochs\n",
        "plt.figure()\n",
        "plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#Plot the accuracy over epochs\n",
        "plt.figure()\n",
        "plt.plot(range(1, len(epoch_accuracies) + 1), [a for a in epoch_accuracies], marker='s')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Test Accuracy Over Epochs\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}